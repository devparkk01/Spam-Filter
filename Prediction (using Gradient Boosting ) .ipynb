{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will do the prediction by loading the model which we trained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "import string \n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "wnLemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=<function clean_text at 0x00000268E2213E18>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"c:/Users/HP/Dropbox/Datasets/SMSSpamCollection.tsv\" , sep = '\\t')  \n",
    "df.columns = ['label' , 'body_text']\n",
    "\n",
    "\n",
    "def get_length_text (text ) :\n",
    "    return len(text) - text.count(\" \") \n",
    "def get_punc_percent (text) :\n",
    "    punc_length = sum([1 for char in text if char in string.punctuation ])\n",
    "    return   round( punc_length / (len(text) - text.count(' ')) , 3 ) *100 \n",
    "\n",
    "def clean_text (text ) :\n",
    "    no_punctuation = \"\".join ([char for char in text if char not in string.punctuation ])\n",
    "    tokenized = re.split (r'[\\W]+' , no_punctuation.lower() )\n",
    "    lemmatized = [wnLemmatizer.lemmatize(word) for word in tokenized if word not in stopwords] \n",
    "    return lemmatized \n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = clean_text)\n",
    "vectorizer.fit(df[\"body_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n",
    "\n",
    "gbmodel =  joblib.load('GradientBoosing.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterYourMail (text) : \n",
    "    length = get_length_text(text)\n",
    "    punc_percent = get_punc_percent(text)\n",
    "    text_series_data = pd.Series(text)  # passing series data because df['body_text']  which we passed  \n",
    "#   while fitting vectorizer is a series data , not  a dataframe .\n",
    "\n",
    "    final_matrix = vectorizer.transform(text_series_data).toarray()\n",
    "    features = pd.DataFrame({'text_length' : [length] , 'punc_percent' : [punc_percent]  })\n",
    "    final_X = pd.concat([features , pd.DataFrame(final_matrix)] , axis = 1 )\n",
    "    prediction =  gbmodel.predict(final_X)[0]\n",
    "    if (prediction == 'ham') : return \"--- It's not a spam message ---  \"\n",
    "    else : return \"--- It's a spam message --- \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- It's a spam message --- \n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"Your free subscription is waiting to be collected. Simply text the password MIX to 85069 to verify.\n",
    "  Get Usher and Britney. FML, PO Box 5249, MK17 92H. 450Ppw 16\"\"\"\n",
    "\n",
    "print (FilterYourMail(message ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- It's not a spam message ---  \n"
     ]
    }
   ],
   "source": [
    "message = \"Are you chilling ? \"\n",
    "\n",
    "print(FilterYourMail(message) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- It's a spam message --- \n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"WINNER!! As a valued network customer you have been selected to receive a $900 prize reward!\n",
    "        To claim call 09061701461. Claim code KL341. Valid 12 hours only. \"\"\"\n",
    "\n",
    "print(FilterYourMail(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- It's not a spam message ---  \n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"Send me your contact number and don't forget to include what was missing in the first place.\n",
    "            This behaviour of yours is not commendable . \"\"\"\n",
    "\n",
    "print(FilterYourMail(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
